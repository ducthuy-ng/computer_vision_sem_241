{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ducthuy-ng/computer_vision_sem_241/blob/main/DLACV_Ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_fpB4GcY9ddj"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqcTCPwX9sy5"
      },
      "source": [
        "## Custom Model for CIFAR-10 classification\n",
        "\n",
        "1. **(3pts)** Create a Resnet-18 or RegNet-Y or MobileNet for CIFAR-10 classification\n",
        "\n",
        "- (a) Train model from scratch without using pretrained weights from ImageNet; evaluate.\n",
        "\n",
        "- (b) Train model using pretrained weights from ImageNet (fine-tune transfer learning); evaluate. The performance must higher than results from (a). Provide discussion for the results.\n",
        "\n",
        "2. **(4pts)** Repeat requirements from (1-a) and (1-b) but add data augmentation with methods from https://github.com/albumentations-team/albumentations . The performance must higher than results from (1) Provide discussion for the results.\n",
        "\n",
        "3. **(3pts)** Find best learning rate for models from (1) and (2) with learning rate finder from https://github.com/surmenok/keras_lr_finder or other similar approaches. Provide discussion for the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For PyTorch-based coding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True, num_workers=2, generator=torch.Generator(device)\n",
        ")\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: DataLoader,\n",
        "    test_dataloader: DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    epoch_nums=10,\n",
        ") -> None:\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(epoch_nums):\n",
        "        # Train mode\n",
        "        epoch_loss = torch.tensor(0.0).to(device)\n",
        "        epoch_accuracy = torch.tensor(0.0).to(device)\n",
        "\n",
        "        for inputs, labels in train_dataloader:\n",
        "            inputs: torch.Tensor = inputs.to(device)\n",
        "            labels: torch.Tensor = labels.to(device)\n",
        "            outputs: torch.Tensor = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            epoch_loss += loss\n",
        "\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "            epoch_accuracy += (preds == labels).sum()\n",
        "\n",
        "        print(epoch_loss, epoch_accuracy)\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model from scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model pre-trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Placeholder storage has not been allocated on MPS device!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m RMSprop(pretrained_model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_nums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, criterion, optimizer, epoch_nums)\u001b[0m\n\u001b[1;32m     16\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m epoch_accuracy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.6/envs/computer_vision/lib/python3.12/site-packages/torch/utils/data/dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.6/envs/computer_vision/lib/python3.12/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.6/envs/computer_vision/lib/python3.12/site-packages/torch/utils/data/dataloader.py:990\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loader):\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mprefetch_factor\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.6/envs/computer_vision/lib/python3.12/site-packages/torch/utils/data/dataloader.py:601\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collate_fn \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mcollate_fn\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_sampler)\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_seed \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mpersistent_workers\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
          ]
        }
      ],
      "source": [
        "from torch.optim.rmsprop import RMSprop\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "pretrained_model = resnet18(weights=\"IMAGENET1K_V1\")\n",
        "for param in pretrained_model.parameters():\n",
        "    param.requires_grad = False\n",
        "pretrained_model.fc = torch.nn.Linear(in_features=512, out_features=len(trainset.targets))\n",
        "pretrained_model.to(device)\n",
        "\n",
        "criterion = CrossEntropyLoss().to(device)\n",
        "optimizer = RMSprop(pretrained_model.fc.parameters(), lr=0.1)\n",
        "train(pretrained_model, trainloader, testloader, criterion, optimizer, epoch_nums=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOCL4TdDmANqK5UeDd0RyIN",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "computer_vision",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
